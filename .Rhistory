total_tdm
##Convert to a matrix
total_m <- as.matrix(total_tdm)
##Print the dimensions
dim(total_m)
##Check out a part of the matrix[terms, by docs]
total_m[100:105, 1:2]
## Let's calculate the term frequency
term_frequency <- rowSums(total_m)
## Sort the terms in descending order
term_frequency <- sort(term_frequency, decreasing = TRUE)
##Check out the top 10
term_frequency[1:10]
##Now we will make total into a VectorSource.
translated_source <- VectorSource(translated)
##Now we will turn VectorSource into a volatile corpus
translated_corpus <- VCorpus(translated_source)
translated_corpus
##We can review a single document here with subsetting
translated_corpus[[2]]
##We can review the actual text from Argentina with a double subset
translated_corpus[[2]][1]
##Let's create a custom stopwords list
mystops <- c(stopwords('english'), "law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, mystops)
return(corpus)
}
##apply it
clean_corp_trans <- clean_corpus(total_corpus)
##print a cleaned doc
clean_corp_trans[[2]][1]
## Create a Document Term Matrix (each document in the corpus represented as a row)
translated_dtm <-DocumentTermMatrix(clean_corp_trans)
##print info
translated_dtm
##Create a matrix version
translated_m <-as.matrix(translated_dtm)
##Check out the dimensions
dim(translated_m)
##Look at Argentina and Australia, terms 2281-2285
translated_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
translated_tdm <- TermDocumentMatrix(clean_corp_trans)
##take a look
translated_tdm
##Convert to a matrix
translated_m <- as.matrix(translated_tdm)
##Print the dimensions
dim(translated_m)
##Check out a part of the matrix[terms, by docs]
translated_m[100:105, 1:2]
## Let's calculate the term frequency
term_frequency_trans <- rowSums(translated_m)
## Sort the terms in descending order
term_frequency_trans <- sort(term_frequency_trans, decreasing = TRUE)
##Check out the top 10
term_frequency_trans[1:10]
##apply it
clean_corp_trans <- clean_corpus(translated_corpus)
## Create a Document Term Matrix (each document in the corpus represented as a row)
translated_dtm <-DocumentTermMatrix(clean_corp_trans)
##print info
translated_dtm
##Create a matrix version
translated_m <-as.matrix(translated_dtm)
##Check out the dimensions
dim(translated_m)
##Look at Argentina and Australia, terms 2281-2285
translated_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
translated_tdm <- TermDocumentMatrix(clean_corp_trans)
##take a look
translated_tdm
##Convert to a matrix
translated_m <- as.matrix(translated_tdm)
## Let's calculate the term frequency
term_frequency_trans <- rowSums(translated_m)
## Sort the terms in descending order
term_frequency_trans <- sort(term_frequency_trans, decreasing = TRUE)
##Check out the top 10
term_frequency_trans[1:10]
##Make a barplot of the top ten terms
barplot(term_frequency_trans[1:10], beside = TRUE, ylim = range(pretty(c(0, term_frequency_trans[1:10]) termcol = "tan", las = 2)
##Make a barplot of the top ten terms
barplot(term_frequency_trans[1:10], beside = TRUE, ylim = range(pretty(c(0, term_frequency_trans[1:10]))), termcol = "tan", las = 2)
##Let's play with wordclouds...
library(wordcloud)
word_freqs <-data.frame(term = names(term_frequency), num = term_frequency)
word_freqs_trans <-data.frame(term = names(term_frequency_trans), num = term_frequency_trans)
wordcloud(word_freqs_trans$term, word_freqs_trans$num, max.words = 100, colors = c("springgreen", "slategrey", "blue", "orange1"))
##In the second part of the TM tutorial in DataCamp, there is a commonality.cloud and comparison.cloud function we can use to show differences between corpuses by collapsing each corpus into one document and comparing.
##We can also do pyramid.plots here to compare corpuses.
##Let's start building different corpuses for comparison. Start with translated texts, then English, French, Spanish, Arabic
##Clean English data frame
library(tm)
##Now we will make total into a VectorSource.
English_source <- VectorSource(English)
##Now we will turn VectorSource into a volatile corpus
English_corpus <- VCorpus(English_source)
English_corpus
##We can review a single document here with subsetting
English_corpus[[2]]
##Let's create a custom stopwords list
mystops <- c(stopwords('english'), "law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, mystops)
return(corpus)
}
##apply it
clean_corp_English <- clean_corpus(English_corpus)
##print a cleaned doc
clean_corp_English[[2]][2]
## Create a Document Term Matrix (each document in the corpus represented as a row)
English_dtm <-DocumentTermMatrix(clean_corp_English)
##print info
English_dtm
##Create a matrix version
English_m <-as.matrix(English_dtm)
##Check out the dimensions
dim(English_m)
##Look at Australia and Botswana, terms 2281-2285
English_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
English_tdm <- TermDocumentMatrix(clean_corp_English)
##take a look
English_tdm
##Convert to a matrix
English_m <- as.matrix(English_tdm)
##Print the dimensions
dim(English_m)
##Check out a part of the matrix[terms, by docs]
English_m[100:105, 1:2]
## Let's calculate the term frequency
term_frequency_English <- rowSums(English_m)
## Sort the terms in descending order
term_frequency_English <- sort(term_frequency_English, decreasing = TRUE)
##Check out the top 10
term_frequency_English[1:10]
##Make a barplot of the top ten terms
barplot(term_frequency[1:10], ylim = range(pretty(c(0, term_frequency_trans[1:10]))), las = 2)
##Make a barplot of the top ten terms
barplot(term_frequency_English[1:10], ylim = range(pretty(c(0, term_frequency_trans[1:10]))), las = 2)
##Let's play with wordclouds...
library(wordcloud)
word_freqs_English <-data.frame(term = names(term_frequency_English), num = term_frequency_English)
wordcloud(word_freqs_English$term, word_freqs_English$num, max.words = 100, colors = c("purple", "slategrey", "blue", "orange1"))
##In the second part of the TM tutorial in DataCamp, there is a commonality.cloud and comparison.cloud function we can use to show differences between corpuses by collapsing each corpus into one document and comparing.
##We can also do pyramid.plots here to compare corpuses.
##Let's start building different corpuses for comparison. Start with translated texts, then English, French, Spanish, Arabic
##Clean French dataframe
library(tm)
##Now we will make total into a VectorSource.
total_source_French <- VectorSource(French)
##Now we will make French into a VectorSource.
French_source <- VectorSource(French)
##Now we will turn VectorSource into a volatile corpus
French_corpus_ <- VCorpus(French_source)
French_corpus
##Now we will make French into a VectorSource.
French_source <- VectorSource(French)
##Now we will turn VectorSource into a volatile corpus
French_corpus <- VCorpus(French_source)
French_corpus
##Let's create a custom stopwords list
mystops <- c(stopwords('english'), "law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis")
##apply it
clean_corp_French <- clean_corpus(French_corpus)
## Create a Document Term Matrix (each document in the corpus represented as a row)
French_dtm <-DocumentTermMatrix(clean_corp_French)
##print info
French_dtm
##Create a matrix version
French_m <-as.matrix(French_dtm)
##Check out the dimensions
dim(French_m)
##Look at Belgium and Cameroon, terms 2281-2285
French_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
French_tdm <- TermDocumentMatrix(clean_corp_French)
##take a look
French_tdm
##Convert to a matrix
French_m <- as.matrix(French_tdm)
##Print the dimensions
dim(French_m)
##Check out a part of the matrix[terms, by docs]
French_m[100:105, 1:2]
## Let's calculate the term frequency
term_frequency_French <- rowSums(French_m)
## Sort the terms in descending order
term_frequency_French <- sort(term_frequency_French, decreasing = TRUE)
##Check out the top 10
term_frequency_French[1:10]
##Make a barplot of the top ten terms
barplot(term_frequency_French[1:10], ylim = range(pretty(c(0, term_frequency_French[1:10]))), las = 2)
##Let's play with wordclouds...
library(wordcloud)
word_freqs_French <-data.frame(term = names(term_frequency_French), num = term_frequency_French)
wordcloud(word_freqs_French$term, word_freqs_French$num, max.words = 100, colors = c("black", "slategrey", "blue", "orange1"))
##Clean Spanish data frame
library(tm)
##Now we will make total into a VectorSource.
Spanish_source <- VectorSource(Spanish)
##Now we will turn VectorSource into a volatile corpus
Spanish_corpus <- VCorpus(Spanish_source)
Spanish_corpus
##Let's create a custom stopwords list
mystops <- c(stopwords('english'), "law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, mystops)
return(corpus)
}
##apply it
Spanish_corp <- clean_corpus(Spanish_corpus)
## Create a Document Term Matrix (each document in the corpus represented as a row)
Spanish_dtm <-DocumentTermMatrix(Spanish_corp)
##print info
Spanish_dtm
##Create a matrix version
Spanish_m <-as.matrix(Spanish_dtm)
##Check out the dimensions
dim(Spanish_m)
##Look at Argentina and Australia, terms 2281-2285
Spanish_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
Spanish_tdm <- TermDocumentMatrix(Spanish_corp)
##take a look
Spanish_tdm
##Convert to a matrix
Spanish_m <- as.matrix(Spanish_tdm)
##Print the dimensions
dim(Spanish_m)
##Check out a part of the matrix[terms, by docs]
Spanish_m[100:105, 1:2]
## Let's calculate the term frequency
term_frequency_Spanish <- rowSums(total_Spanish)
## Let's calculate the term frequency
term_frequency_Spanish <- rowSums(Spanish_m)
## Sort the terms in descending order
term_frequency_Spanish <- sort(term_frequency_Spanish, decreasing = TRUE)
##Check out the top 10
term_frequency_Spanish[1:10]
barplot(term_frequency_Spanish[1:10], ylim = range(pretty(c(0, term_frequency_Spanish[1:10]))), las = 2)
##Let's play with wordclouds...
library(wordcloud)
word_freqs_Spanish <-data.frame(term = names(term_frequency_Spanish), num = term_frequency_Spanish)
wordcloud(word_freqs_Spanish$term, word_freqs_Spanish$num, max.words = 100, colors = c("black", "slategrey", "blue", "orange1"))
##Clean Arabic data frame
library(tm)
##Now we will make total into a VectorSource.
Arabic_source <- VectorSource(Arabic)
##Now we will turn VectorSource into a volatile corpus
Arabic_corpus <- VCorpus(Arabic_source)
Arabic_corpus
##Let's create a custom stopwords list
mystops <- c(stopwords('english'), "law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, mystops)
return(corpus)
}
##apply it
Spanish_clean_corp <- clean_corpus(Spanish_corpus)
## Create a Document Term Matrix (each document in the corpus represented as a row)
Spanish_dtm <-DocumentTermMatrix(Spanish_clean_corp)
##apply it
Arabic_clean_corp <- clean_corpus(Arabic_corpus)
##print a cleaned doc
Arabic_clean_corp[[2]][1]
## Create a Document Term Matrix (each document in the corpus represented as a row)
Arabic_dtm <-DocumentTermMatrix(Arabic_clean_corp)
##print info
Arabic_dtm
##Create a matrix version
Arabic_m <-as.matrix(Arabic_dtm)
##Check out the dimensions
dim(Arabic_m)
##Look at Argentina and Australia, terms 2281-2285
Arabic_m[1:2, 2281:2285]
##Look at the transpose of the document term matrix, ie. the term document matrix (TDM)
##This will have terms in the first column and documents across the top (this tends to be easier to use
##for text analysis)
Arabic_tdm <- TermDocumentMatrix(Arabic_clean_corp)
##take a look
Arabic_tdm
##Convert to a matrix
Arabic_m <- as.matrix(Arabic_tdm)
##Print the dimensions
dim(Arabic_m)
##Check out a part of the matrix[terms, by docs]
Arabic_m[100:105, 1:2]
## Let's calculate the term frequency
Arabic_term_frequency <- rowSums(Arabic_m)
## Sort the terms in descending order
Arabic_term_frequency <- sort(Arabic_term_frequency, decreasing = TRUE)
##Check out the top 10
Arabic_term_frequency[1:10]
##Make a barplot of the top ten terms
barplot(Arabic_term_frequency[1:10], ylim = range(pretty(c(0, Arabic_term_frequency[1:10]))), las = 2)
##Let's play with wordclouds...
library(wordcloud)
word_freqs <-data.frame(term = names(Arabic_term_frequency), num = Arabic_term_frequency)
wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = c("black", "slategrey", "blue", "orange1"))
##Play with word associations
library(tm)
associations <-findAssocs(total_tdm, "disability", 0.2)
##View them
associations
##put them in a data frame
associations_df <-data.frame(list_vec2df(associations)[, 2:3])
##put them in a data frame
library(qdap)
associations_df <-data.frame(list_vec2df(associations)[, 2:3])
library(rJava)
##put them in a data frame
?packages
##put them in a data frame
install.packages(rJava)
##put them in a data frame
??rJava
library(tm)
library(qplot)
install.packages("qplot")
library(qplot)
install.packages("qdap")
library(qdap)
install.packages(rJava)
install.packages("rJava")
install.packages(qdap)
install.packages("qdap")
library(qdap)
system("java -version")
system("r -version")
install.packages('rJava', type='source')
install.packages()
install.packages("qdap")
library(qdap)
library(rJava)
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
install.packages(c("bayesplot", "bindr", "bindrcpp", "bridgesampling", "brms", "Brobdingnag", "broom", "cluster", "colourpicker", "curl", "devtools", "dplyr", "DT", "dygraphs", "forcats", "foreign", "ggplot2", "git2r", "gtools", "haven", "highr", "hms", "htmlwidgets", "httpuv", "igraph", "inline", "irlba", "ISOcodes", "lazyeval", "loo", "MASS", "Matrix", "mgcv", "miniUI", "modelr", "munsell", "mvtnorm", "nlme", "openssl", "packrat", "pdftools", "PKI", "plogr", "psych", "purrr", "quanteda", "RcppArmadillo", "RCurl", "readxl", "reshape2", "reticulate", "rjson", "rlang", "rmarkdown", "rpart", "rsconnect", "rstan", "rstantools", "Rttf2pt1", "selectr", "shiny", "shinyjs", "shinystan", "sn", "sourcetools", "StanHeaders", "statnet.common", "stm", "streamR", "stringi", "survival", "tibble", "tidyr", "tidyselect", "tm", "viridisLite", "withr", "xml2", "xts", "zoo"))
library(tm)
library(qdap)
install.packages("RWeka")
library(RWeka)
if(Sys.getenv("JAVA_HOME")!=""){
Sys.setenv(JAVA_HOME="")
}
library(rJava)
install.packages("rJava")
if(Sys.getenv("JAVA_HOME")!=""){
Sys.setenv(JAVA_HOME="")
}
library(rJava)
library(stm)
install.packages(stm)
install.packages("stm")
library(stm)
?textir
??textir
temp <- textProcessor(documents =total$text, meta = NULL, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE,
stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1, language = "en", verbose = TRUE, customstopwords = c("law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis", "title", "decree"))
install.packages("tm")
install.packages("SnowballC")
temp <- textProcessor(documents =total$text, meta = NULL, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE,
stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1, language = "en", verbose = TRUE, customstopwords = c("law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis", "title", "decree"))
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
out <- prepDocuments(docs, vocab, meta)
## Let's fit it to our 17 topic model
totalPrevFit <- stm(documents = out$documents, vocab = out$vocab,
K = 17, max.em.its = 200, data = out$meta, init.type = "Spectral")
##Let's pick a model
?selectModel
totalSelect <-selectModel(out$documents, out$vocab, K = 17, max.em.its = 200, data = out$meta,
runs = 50, seed = 12345)
totalSelect <-selectModel(out$documents, out$vocab, K = 17, max.em.its = 300, data = out$meta,
runs = 50, seed = 12345)
totalSelect <-selectModel(out$documents, out$vocab, K = 17, max.em.its = 400, data = out$meta,
runs = 50, seed = 12345)
?plotModels
totalSelectPlot <- plotModels(models, xlab = "Semantic Coherence", ylab = "Exclusivity", labels = 1:length(models$runout), pch= NULL, legend.position = "topleft")
totalSelectPlot <- plotModels(totalSelect, xlab = "Semantic Coherence", ylab = "Exclusivity", labels = 1:length(models$runout), pch= NULL, legend.position = "topleft")
totalSelectPlot <- plotModels(totalSelect, xlab = "Semantic Coherence", ylab = "Exclusivity", labels = 1:length(totalSelect$runout), pch= NULL, legend.position = "topleft")
selectedmodel7 <-totalSelect$runout[[7]]
?labelTopics
total7 <- labelTopics(model, topics = NULL, n = 10)
total7 <- labelTopics(selectedmodel7, topics = NULL, n = 10)
total7
selectedmodel1 <-totalSelect$runout[[1]]
total1 <- labelTopics(selectedmodel1, topics = NULL, n = 10)
total1
##fewer country specifc terms here,so let's work with this one.
?plot.STM
plot(total1, type = "hist")
plot(total1, type = c("summary"), n = NULL, labeltype = c("prob"), main = "Summary of Topics Across Documents")
plot(selectedmodel1, type = c("summary"), n = NULL, labeltype = c("prob"), main = "Summary of Topics Across Documents")
## We can come back to this later if we want to plot perspectives (one topic compared to another)
##Let's try plotting quotes from documents representative of each topic
?findThoughts
thoughts1 <-findThoughts(selectedmodel1)
thoughts1
thoughts1 <-findThoughts(selectedmodel1, texts = total$text, topics = c(1:17), n = 3)
thoughts1
thoughts1 <-findThoughts(selectedmodel1, texts = total$text, topics = c(1), n = 3)
thoughts1
##This returns a document that is most highly associated with the specified topic.  Interestingly,
##Poland is most representative of the first topic.
##Once we have meta-data, we can try the plot.estimateEffect function here, which will deliver a regression table
?plotQuote
plotQuote(sentences = 1:3, width = 30, text.cex = 1, main = "plotQuote for Topic 1")
plotQuote(thoughts1, width = 30, main = "Topic 1")
plotQuote(thoughts1, width = 30, ylim = 30 main = "Topic 1")
plotQuote(thoughts1, width = 30, ylim = 30, main = "Topic 1")
plotQuote(thoughts1, sentences = 3, width = 30, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 100, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 500, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 1000, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 800, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 700, main = "Topic 1")
plotQuote(thoughts1, width = 30, maxwidth = 600, main = "Topic 1")
## Let's fit it to our 17 topic model
totalPrevFit <- stm(documents = out$docs, vocab = out$vocab,
K = 17, max.em.its = 200, data = out$meta, init.type = "Spectral")
##Let's get things running in STM again
temp <- textProcessor(documents =total$text, meta = NULL, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE,
stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1, language = "en", verbose = TRUE, customstopwords = c("law", "article", "section", "chapter", "append", "sub-sect", "paragraph", "titl", "author",
"act", "decre", "etc", "herein", "update", "sen",  "part", "provis", "title", "decree"))
out <- prepDocuments(temp$documents, temp$vocab, temp$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
## Let's fit it to our 17 topic model
totalPrevFit <- stm(documents = out$documents, vocab = out$vocab,
K = 17, max.em.its = 200, data = out$meta, init.type = "Spectral")
totalSelect <-selectModel(out$documents, out$vocab, K = 17, max.em.its = 400, data = out$meta,
runs = 50, seed = 12345)
totalSelectPlot <- plotModels(totalSelect, xlab = "Semantic Coherence", ylab = "Exclusivity", labels = 1:length(totalSelect$runout), pch= NULL, legend.position = "topleft")
total7 <- labelTopics(selectedmodel7, topics = NULL, n = 10)
total7
selectedmodel1 <-totalSelect$runout[[1]]
total1 <- labelTopics(selectedmodel1, topics = NULL, n = 10)
total1
plot(selectedmodel1, type = c("summary"), n = NULL, labeltype = c("prob"), main = "Summary of Topics Across Documents")
thoughts1 <-findThoughts(selectedmodel1, texts = total$text, topics = c(1), n = 3)
thoughts1
plotQuote(thoughts1, width = 30, maxwidth = 600, main = "Topic 1")
plot(selectedmodel1, type = "labels", topics = 1:17)
plot(selectedmodel1, type = "labels", topics = 1:5)
plot(selectedmodel1, type = "labels", topics = 5:10)
plot(selectedmodel1, type = "labels", topics = 6:11)
plot(selectedmodel1, type = "labels", topics = 12:17)
##We can look at the best topics with the topicQuality function
topicQuality(model = selectedmodel1, documents = docs)
thoughts1 <-findThoughts(selectedmodel1, texts = shortdoc, n = 3, topics = 1)
thoughts1
plotQuote(thoughts1, width = 40, maxwidth = 600, main = "Topic 1")
thoughts1 <-findThoughts(selectedmodel1, texts = shortdoc, n = 3, topics =1)$docs[[1]]
plotQuote(thoughts1, width = 40, maxwidth = 600, main = "Topic 1")
thoughts1 <-findThoughts(selectedmodel1, texts=shortdoc, n = 3, topics =1)$docs[[1]]
plotQuote(thoughts1$doc[[1]], width = 40, maxwidth = 600, main = "Topic 1")
thoughts2 <-findThoughts(selectedmodel1, texts=shortdoc, n = 3, topics =2)$docs[[1]]
thoughts2 <-findThoughts(selectedmodel1, n = 3, topics =2)$docs[[1]]
thoughts2
thoughts2
##This returns a document that is most highly associated with the specified topic.  Interestingly,
##Poland is most representative of the first topic.
##Once we have meta-data, we can try the plot.estimateEffect function here, which will deliver a regression table
?plotQuote
plotQuote(selectedmodel1$doc[[1]], width = 40, maxwidth = 600, main = "Topic 1")
